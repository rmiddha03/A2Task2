{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMc5SbzwmHvP0N6BGyduWVP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmiddha03/A2Task2/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZhUWqGGIjxE"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import urllib.parse\n",
        "import string\n",
        "\n",
        "# ---------- HELPER FUNCTION TO EXTRACT ARTICLE TITLE ----------\n",
        "def extract_article_title(input_string):\n",
        "    input_string = input_string.strip()\n",
        "    if input_string.startswith(\"http\"):\n",
        "        # Remove any trailing slash, then extract last part\n",
        "        title = input_string.rstrip(\"/\").split(\"/\")[-1]\n",
        "        # Remove any trailing punctuation (like a period)\n",
        "        title = title.strip(string.punctuation)\n",
        "        return title\n",
        "    return input_string.strip().strip(string.punctuation)\n",
        "\n",
        "# ---------- USER INPUTS ----------\n",
        "article1_input = input(\"Enter the first Wikipedia article title or URL: \")\n",
        "article2_input = input(\"Enter the second Wikipedia article title or URL: \")\n",
        "start_date_str = input(\"Enter start date (YYYY-MM-DD): \")\n",
        "end_date_str   = input(\"Enter end date (YYYY-MM-DD): \")\n",
        "\n",
        "article1 = extract_article_title(article1_input)\n",
        "article2 = extract_article_title(article2_input)\n",
        "\n",
        "start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
        "end_date   = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n",
        "\n",
        "# ---------- HELPER FUNCTION TO FETCH PAGEVIEWS ----------\n",
        "def get_daily_pageviews(article_title, start_dt, end_dt):\n",
        "    # Replace spaces with underscores, then URL-encode the article title\n",
        "    article_title = article_title.replace(\" \", \"_\")\n",
        "    article_title = urllib.parse.quote(article_title)\n",
        "\n",
        "    start_str = start_dt.strftime(\"%Y%m%d\")\n",
        "    end_str   = end_dt.strftime(\"%Y%m%d\")\n",
        "\n",
        "    base_url = \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article\"\n",
        "    # Use 'user' as the agent (not 'page')\n",
        "    url = f\"{base_url}/en.wikipedia.org/all-access/user/{article_title}/daily/{start_str}/{end_str}\"\n",
        "\n",
        "    headers = {\n",
        "        \"User-Agent\": \"WikiPageviewsFetcher/1.0 (nipunbhalla80@gmail.com)\"\n",
        "    }\n",
        "\n",
        "    resp = requests.get(url, headers=headers)\n",
        "    if resp.status_code != 200:\n",
        "        print(f\"Error fetching data for {article_title}: {resp.status_code}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    data = resp.json()\n",
        "    if 'items' not in data:\n",
        "        print(f\"No data found for {article_title}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    all_records = []\n",
        "    for item in data['items']:\n",
        "        date_str = item['timestamp'][:8]  # YYYYMMDD\n",
        "        date_obj = datetime.strptime(date_str, \"%Y%m%d\")\n",
        "        views    = item['views']\n",
        "        all_records.append((date_obj, views))\n",
        "\n",
        "    df = pd.DataFrame(all_records, columns=['date', 'views'])\n",
        "    df.sort_values('date', inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    return df\n",
        "\n",
        "# ---------- FETCH DATA ----------\n",
        "df1 = get_daily_pageviews(article1, start_date, end_date)\n",
        "df2 = get_daily_pageviews(article2, start_date, end_date)\n",
        "\n",
        "# Rename columns so we can merge\n",
        "df1.rename(columns={'views': f\"{article1}_views\"}, inplace=True)\n",
        "df2.rename(columns={'views': f\"{article2}_views\"}, inplace=True)\n",
        "\n",
        "if df1.empty or df2.empty:\n",
        "    print(\"One or both dataframes are empty. Check article titles and date range.\")\n",
        "else:\n",
        "    merged_df = pd.merge(df1, df2, on='date', how='outer')\n",
        "    merged_df.sort_values('date', inplace=True)\n",
        "    merged_df.fillna(0, inplace=True)\n",
        "\n",
        "    # ---------- PLOT ----------\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(merged_df['date'], merged_df[f\"{article1}_views\"], label=article1)\n",
        "    plt.plot(merged_df['date'], merged_df[f\"{article2}_views\"], label=article2)\n",
        "    plt.title(\"Wikipedia Pageviews Comparison\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Daily Views\")\n",
        "    plt.legend()\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # ---------- DISPLAY ----------\n",
        "    print(merged_df.head(30))\n"
      ]
    }
  ]
}